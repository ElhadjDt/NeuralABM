{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SIR calibration demo\n",
    "\n",
    "## Learning parameters from a single dataset\n",
    "\n",
    "In this Jupyter notebook, we will demonstrate the basics of the methodology of calibrating a single dataset without adding too much computational complexity. The idea is just to set up a simple ODE model, calibrate its parameters, and plot the results, illustrating the basic idea of the methodology. With this approach, you can already learn system parameters from a single observation time series. However, the methodology can also be used to learn parameters from multiple observations, or to train a neural network to learn different parameters from different observations. These will be demonstrated in future notebooks and models as I expand the methodology.\n",
    "\n",
    "Let's get started by first importing the required packages -- make sure they are installed on your system:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f918c5fba68b92e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we import neural network generation function from `include.neural_network`:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "158841514d0bd37b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Local imports\n",
    "from include import neural_net"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0946e5eb76269f4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, let's first generate some synthetic SIR data. The originaal function (as with all models) is located in the `<model_name>/DataGeneration.py` folder. You can adapt these to your own needs, or come up your own structure. For our purposes, lets rewrite the function and reuse it later on for training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63b91509316324b9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_SIR_data(*,\n",
    "        cfg: dict,\n",
    "        num_steps: int,\n",
    "        init_state: torch.tensor,\n",
    "        dt: float = 1\n",
    "):\n",
    "    \"\"\" Forward Euler solver for the SIR model.\"\"\"\n",
    "    data = [init_state]\n",
    "    \n",
    "    for i in range(num_steps):        \n",
    "        \n",
    "        # Recovery rate, which is truncated in time. Since the Heaviside function is not differentiable, \n",
    "        # we use a very steep sigmoid, which looks almost the same\n",
    "        tau = 1 / cfg[\"t_infectious\"] * torch.sigmoid(1000 * (i / cfg[\"t_infectious\"] - 1))\n",
    "        \n",
    "        # Solve the ODE using a forward Euler\n",
    "        data.append(\n",
    "           torch.clip(data[-1] + torch.stack(\n",
    "                [\n",
    "                    (-cfg[\"p_infect\"] * data[-1][0] + cfg[\"sigma\"] * torch.normal(0.0, 1.0, size=(1, )))\n",
    "                    * data[-1][1],\n",
    "                    (cfg[\"p_infect\"] * data[-1][0] - tau + cfg[\"sigma\"] * torch.normal(0.0, 1.0, size=(1, )))\n",
    "                    * data[-1][1],\n",
    "                    tau * data[-1][1],\n",
    "                ]\n",
    "            ) * dt, 0, 1)\n",
    "        )\n",
    "    \n",
    "    # Return the time series\n",
    "    return torch.stack(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d278a61f1f5bacc",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's generate some training data. Let's assume that 1% of the population are infected at time `t=0` and use this as the initial data:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "554f0e0342e7241f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Generate an initial state and run the model:\n",
    "init_state = torch.tensor([0.99, 0.01, 0], dtype=torch.float).reshape(-1, 1)\n",
    "SIR_data = generate_SIR_data(\n",
    "    cfg={\n",
    "        \"t_infectious\": torch.tensor(14),\n",
    "        \"p_infect\": torch.tensor(0.2), \n",
    "        \"sigma\": torch.tensor(0.1)\n",
    "    }, \n",
    "    num_steps=1000, \n",
    "    init_state=init_state,\n",
    "    dt=0.1\n",
    ")\n",
    "print(SIR_data.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "345e62b8beb21436",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's quickly plot the training to see what it looks like. Converting to an xarray dataset makes this more convenient, and also illustrates how the `utopya` DAG framework works -- but this is optional, you can of course just plot using matplotlib directly if you prefer:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54dffb4f022eb449"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "SIR_data_xr = xr.DataArray(data=SIR_data.squeeze(), \n",
    "                           dims=[\"time\", \"kind\"], \n",
    "                           coords={\"time\": np.arange(SIR_data.shape[0]), \"kind\": [\"S\", \"I\", \"R\"]})\n",
    "SIR_data_xr.plot.line(hue=\"kind\");"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "340ace8a9342ca37",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Okay -- looks good. Now let's train a neural network to calibrate just this one single dataset. If you have many different datasets with many different parameters, the procedure would be analogous but with a larger training set. \n",
    "\n",
    "We want to learn two parameters: p_infect and t_infectious. First, let's initialise a neural network to take an (S, I, R) vector as input and output two predicted parameters. We'll use a small net with 3 layers, 20 neurons per layer, and sigmoid activation functions on all layers except the last, where we use the absolute value:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3d75daf1b51305"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "NN = neural_net.NeuralNet(\n",
    "    input_size = SIR_data.shape[1], # We have S, I, R compartments\n",
    "    output_size = 2, # We want to learn 2 parameters\n",
    "    num_layers=3,\n",
    "    nodes_per_layer={\"default\": 20}, \n",
    "    activation_funcs={\"default\": \"sigmoid\", \"layer_specific\": {-1: \"abs\"}}, \n",
    "    biases={\"default\": None}\n",
    ")\n",
    "# All other parameters (optimizer, learning rate etc.) are the default -- see include.neural_net.NeuralNet for details\n",
    "\n",
    "# Store the loss and parameters in an array\n",
    "loss_ts, parameters_ts = [], []"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2c2f298dc3db1e5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "All right -- now let's train, and track the loss and parameters as the neural network trains. We need to decide how often to make a gradient descent step, and with such long time series it's typically best to use a batch size of at least half the length of the time series, so that the neural network gets to see more of the structure of the data before optimising its parameters. The time series is 100 steps long, so let's just set the batch size to 100, for simplicity."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68edb4347b01e8c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Generate some batch slices:\n",
    "batch_size = 1000\n",
    "batches = np.arange(0, SIR_data.shape[0], batch_size)\n",
    "print(batches)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ddfc9e63ac3e24",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following training routine should run in about 60 seconds on a reasonably fast computer:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db1b9b6827b377ea"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "\n",
    "for _ in (pbar:= tqdm.tqdm(range(num_epochs))):\n",
    "            \n",
    "    # Now iterate over the time series in batches and perform a gradient descent step after every batch\n",
    "    for idx in range(len(batches)-1):\n",
    "        \n",
    "        # Initial frame\n",
    "        init_state = SIR_data[idx]\n",
    "        \n",
    "        # Make a prediction from the initial frame\n",
    "        predicted_parameters = NN(init_state.squeeze())\n",
    "        \n",
    "        # Generate a predicted time series with zero noise\n",
    "        predicted_time_series = generate_SIR_data(\n",
    "            cfg={\n",
    "                \"p_infect\": predicted_parameters[0],\n",
    "                \"t_infectious\": predicted_parameters[1],\n",
    "                \"sigma\": torch.tensor(0.0)\n",
    "            }, \n",
    "            num_steps = batch_size - 1,\n",
    "            init_state = init_state, \n",
    "            dt=0.1\n",
    "        )\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = torch.nn.functional.mse_loss(\n",
    "            SIR_data[batches[idx]+1:batches[idx+1]], predicted_time_series[1:]\n",
    "        )\n",
    "        \n",
    "        # Perform a gradient descent step\n",
    "        loss.backward()\n",
    "        NN.optimizer.step()\n",
    "        NN.optimizer.zero_grad()\n",
    "        \n",
    "        # Store prediction and loss\n",
    "        loss_ts.append(loss.clone().detach())\n",
    "        parameters_ts.append(predicted_parameters.clone().detach())\n",
    "    \n",
    "        pbar.set_description(f\"Current loss: {loss}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c7613c4e7fd84a6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's plot the loss to see how it developed:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9f090f04425f326"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(len(loss_ts)), loss_ts)\n",
    "ax.set_yscale('log');"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11f21b0b809169c9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks reasonable -- let's take a look at the evolution of the parameters, again using xarray for simplicity:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba975f2ffebc26bf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters_ts_xr = xr.DataArray(\n",
    "    data=np.array(parameters_ts),\n",
    "    dims=[\"batch\", \"parameter\"],\n",
    "    coords={\"batch\": np.arange(len(parameters_ts)), \"parameter\": [\"p_infect\", \"t_infectious\"]}\n",
    ")\n",
    "\n",
    "parameters_ts_xr.plot.line(hue=\"parameter\");"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "119b294a89f78e2a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that model is moving towards the true values, but very slowly. The problem is that the two parameters to be learned are two orders of magnitude apart, and this means that the gradients are skewed, slowing the training process immensely. We can speed this up by adding scaling factors to the outputs:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0277f7c052cd65a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scaling = torch.tensor([1, 10])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "786fc755774c33ba",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's reset everything and run again:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1eaac7901f111fbf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "NN = neural_net.NeuralNet(\n",
    "    input_size = SIR_data.shape[1],\n",
    "    output_size = 2,\n",
    "    num_layers=3,\n",
    "    nodes_per_layer={\"default\": 20}, \n",
    "    activation_funcs={\"default\": \"sigmoid\", \"layer_specific\": {-1: \"abs\"}}, \n",
    "    biases={\"default\": None}\n",
    ")\n",
    "loss_ts, parameters_ts = [], []\n",
    "\n",
    "for _ in (pbar:= tqdm.tqdm(range(num_epochs))):\n",
    "            \n",
    "    # Now iterate over the time series in batches and perform a gradient descent step after every batch\n",
    "    for idx in range(len(batches)-1):\n",
    "        \n",
    "        # Initial frame\n",
    "        init_state = SIR_data[idx]\n",
    "        \n",
    "        # Make a prediction from the initial frame\n",
    "        predicted_parameters = scaling * NN(init_state.squeeze())\n",
    "        \n",
    "        # Generate a predicted time series with zero noise\n",
    "        predicted_time_series = generate_SIR_data(\n",
    "            cfg={\n",
    "                \"p_infect\": predicted_parameters[0],\n",
    "                \"t_infectious\": predicted_parameters[1],\n",
    "                \"sigma\": torch.tensor(0.0)\n",
    "            }, \n",
    "            num_steps = batch_size - 1,\n",
    "            init_state = init_state, \n",
    "            dt=0.1\n",
    "        )\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = torch.nn.functional.mse_loss(\n",
    "            SIR_data[batches[idx]+1:batches[idx+1]], predicted_time_series[1:]\n",
    "        )\n",
    "        \n",
    "        # Perform a gradient descent step\n",
    "        loss.backward()\n",
    "        NN.optimizer.step()\n",
    "        NN.optimizer.zero_grad()\n",
    "        \n",
    "        # Store prediction and loss\n",
    "        loss_ts.append(loss.clone().detach())\n",
    "        parameters_ts.append(predicted_parameters.clone().detach())\n",
    "    \n",
    "        pbar.set_description(f\"Current loss: {loss}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46e628603115be57",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "loss_ts_xr = xr.DataArray(data=loss_ts, dims=[\"batch\"], coords={\"batch\": np.arange(len(loss_ts))})\n",
    "loss_ts_xr.plot.line(ax=ax)\n",
    "ax.set_yscale('log')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a61393ec634ea615",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters_ts_xr = xr.DataArray(\n",
    "    data=np.array(parameters_ts),\n",
    "    dims=[\"batch\", \"parameter\"],\n",
    "    coords={\"batch\": np.arange(len(parameters_ts)), \"parameter\": [\"p_infect\", \"t_infectious\"]}\n",
    ")\n",
    "\n",
    "parameters_ts_xr.plot.line(col=\"parameter\", sharey=False);"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d54cb32f432e7c22",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This looks much better!\n",
    "\n",
    "Let's plot the joint distribution of all the parameters we've estimated; we can do so using the `joint_2D_ds` function, located in `model_plots.data_ops`:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f37cb766da934c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from model_plots.data_ops import joint_2D_ds\n",
    "\n",
    "joint = joint_2D_ds(parameters_ts_xr, loss_ts_xr, x=\"p_infect\", y=\"t_infectious\")\n",
    "joint.plot(x=\"p_infect\", ax=ax)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7a8ab6452468dd66",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see the single path that the neural network traced out on its way to the minimum at (0.2, 14). To truly get a sense of the joint distribution, we would need to train the neural network multiple times from different initialisations, ideally in parallel. All this is implemented in the utopya version of the model. We recommend you now continue to the main tutorial in the README."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d6be62634ef8c3a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "aa1c314835b4f150"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "venv",
   "language": "python",
   "display_name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
